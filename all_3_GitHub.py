# -*- coding: utf-8 -*-
"""Copy of All@3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19X5-XNNvUjC0Ywy35Uh4AiLCSApIMs9b

# G-DRIVE Mounting
"""

from google.colab import drive
drive.mount('/content/drive')

input_folder = "/content/drive/MyDrive/Colab Notebooks/PDF2/2003"

"""# PDF TO TEXT

Installing **PyPDF**
"""

!pip install PyPDF2

"""**For Printing And Making File**"""

import os
from PyPDF2 import PdfReader

def extract_text_from_pdf(pdf_path, output_text_file):
    reader = PdfReader(pdf_path)
    text = ""

    # Concatenate text from all pages
    for page_num in range(len(reader.pages)):
        page = reader.pages[page_num]
        text += page.extract_text()

    with open(output_text_file, "w", encoding="utf-8") as text_file:
        text_file.write(text)

def process_pdfs(input_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)

    for pdf_file in os.listdir(input_folder):
        if pdf_file.endswith(".pdf"):
            pdf_path = os.path.join(input_folder, pdf_file)
            output_text_file = os.path.join(output_folder, os.path.splitext(pdf_file)[0] + ".txt")

            extract_text_from_pdf(pdf_path, output_text_file)

# Specify the paths
output_folder = "/content/TXT"

process_pdfs(input_folder, output_folder)

"""# OpenNyai"""

!pip install -U opennyai

!conda install cudatoolkit==<your_cuda_version> #### E.g. cudatoolkit==11.2
!pip install cupy-cuda<your_cuda_version> ##### E.g. cupy-cuda112

"""# Processing TEXT File"""

import os
import json
from opennyai import Pipeline
from opennyai.utils import Data

def process_text_files(input_folderRR, output_folderRR):
    if not os.path.exists(output_folderRR):
        os.makedirs(output_folderRR)

    # If you have access to GPU, set this to True; otherwise, set it to False
    use_gpu = True

    # Choose which of the AI models you want to run from the 3 models 'NER', 'Rhetorical_Role','Summarizer'.
    # For example, if just Named Entity is of interest, then just select 'NER'
    pipeline = Pipeline(components=['Rhetorical_Role'], use_gpu=use_gpu, verbose=True)


    for txt_file in os.listdir(input_folderRR):
        if txt_file.endswith(".txt"):
            txt_path = os.path.join(input_folderRR, txt_file)
            output_json_file = os.path.join(output_folderRR, os.path.splitext(txt_file)[0] + "_output.json")

            with open(txt_path, 'r', encoding='utf-8') as file:
                text = file.read()

            # Create a Data object for data preprocessing before running ML models
            texts_to_process = [text]
            data = Data(texts_to_process)

            # Run the pipeline on the single file
            results = pipeline(data)

            # Save the results to a JSON file
            with open(output_json_file, 'w', encoding='utf-8') as json_file:
                json.dump(results, json_file)

# Specify the paths
input_folderRR = output_folder
output_folderRR = "/content/RRoutput"
process_text_files(input_folderRR, output_folderRR)

import os
import json
from opennyai import Pipeline
from opennyai.utils import Data

def process_text_files(input_folderNER, output_folderNER):
    if not os.path.exists(output_folderNER):
        os.makedirs(output_folderNER)

    # If you have access to GPU, set this to True; otherwise, set it to False
    use_gpu = True

    # Choose which of the AI models you want to run from the 3 models 'NER', 'Rhetorical_Role','Summarizer'.
    # For example, if just Named Entity is of interest, then just select 'NER'
    pipeline = Pipeline(components=['NER'], use_gpu=use_gpu, verbose=True)


    for txt_file in os.listdir(input_folderNER):
        if txt_file.endswith(".txt"):
            txt_path = os.path.join(input_folderNER, txt_file)
            output_json_file = os.path.join(output_folderNER, os.path.splitext(txt_file)[0] + "_output.json")

            with open(txt_path, 'r', encoding='utf-8') as file:
                text = file.read()

            # Create a Data object for data preprocessing before running ML models
            texts_to_process = [text]
            data = Data(texts_to_process)

            # Run the pipeline on the single file
            results = pipeline(data)

            # Save the results to a JSON file
            with open(output_json_file, 'w', encoding='utf-8') as json_file:
                json.dump(results, json_file)

# Specify the paths
input_folderNER = output_folder
output_folderNER = "/content/NERoutput"

process_text_files(input_folderNER, output_folderNER)

"""# Json To CSV

To Retrive Rhetorical Roles to CSV file.
"""

import os
import json
import csv

def process_json_files(JSON_input_folderRR, CSV_output_folderRR):
    if not os.path.exists(CSV_output_folderRR):
        os.makedirs(CSV_output_folderRR)

    for json_file in os.listdir(JSON_input_folderRR):
        if json_file.endswith(".json"):
            json_path = os.path.join(JSON_input_folderRR, json_file)
            output_csv_file = os.path.join(CSV_output_folderRR, os.path.splitext(json_file)[0] + ".csv")

            with open(json_path, 'r', encoding='utf-8') as file:
                json_data = json.load(file)

            with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:
                csv_writer = csv.writer(csv_file)
                csv_writer.writerow(['Text', 'Labels'])

                for entry in json_data:
                    annotations = entry.get('annotations', [])

                    for annotation in annotations:
                        text_data = annotation.get('text', '')
                        label_data = annotation.get('labels', [])

                        csv_writer.writerow([text_data, label_data])

            print(f'Data has been written to {output_csv_file}')

# Specify the paths
JSON_input_folderRR = output_folderRR
CSV_output_folderRR = "/content/RR_CSV"

process_json_files(JSON_input_folderRR, CSV_output_folderRR)

"""To NER to CSV file."""

import os
import csv
import json

def extract_information(json_data):
    entities = {}
    legal_provisions = {}
    judges_and_location = {}

    for entry in json_data:
        for annotation in entry.get('annotations', []):
            for entity in annotation.get('entities', []):
                normalized_name = entity.get('normalized_name', None)
                text = entity.get('text', None)
                labels = entity.get('labels', [])

                if normalized_name is not None:
                    if normalized_name not in entities:
                        entities[normalized_name] = {'text': [], 'labels': []}
                    entities[normalized_name]['text'].append(text)
                    entities[normalized_name]['labels'].extend(labels)

                if 'PROVISION' in labels:
                    if normalized_name not in legal_provisions:
                        legal_provisions[normalized_name] = []
                    legal_provisions[normalized_name].append(text)

                if 'JUDGE' in labels:
                    if normalized_name not in judges_and_location:
                        judges_and_location[normalized_name] = []
                    judges_and_location[normalized_name].append(text)

            for entity in annotation.get('entities', []):
                if 'GPE' in entity.get('labels', []):
                    judges_and_location['Location'] = entity.get('text', None)
                elif 'DATE' in entity.get('labels', []):
                    judges_and_location['Date'] = entity.get('text', None)

    return entities, legal_provisions, judges_and_location

def save_to_csv(file_path, data):
    with open(file_path, 'w', newline='') as csvfile:
        fieldnames = ['Entity', 'Labels', 'Text']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()

        for entity, info in data.items():
            writer.writerow({'Entity': entity, 'Labels': info['labels'], 'Text': info['text']})

def process_json_files(JSONinput_folderNER, JSONoutput_folderNER):
    os.makedirs(JSONoutput_folderNER, exist_ok=True)

    for json_file in os.listdir(JSONinput_folderNER):
        if json_file.endswith(".json"):
            json_path = os.path.join(JSONinput_folderNER, json_file)

            with open(json_path, 'r') as file:
                json_data = json.load(file)

            entities, _, _ = extract_information(json_data)

            csv_output_path = os.path.join(JSONoutput_folderNER, os.path.splitext(json_file)[0] + ".csv")
            save_to_csv(csv_output_path, entities)

# Specify the paths
JSONinput_folderNER = output_folderNER
JSONoutput_folderNER = "/content/NER_CSV"

process_json_files(JSONinput_folderNER, JSONoutput_folderNER)